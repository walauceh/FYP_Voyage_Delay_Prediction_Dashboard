{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3211e8b",
   "metadata": {},
   "source": [
    "# Combine multi year data files and integrate into 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "394ee202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from swifter) (2.2.3)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\ee hann\\appdata\\roaming\\python\\python312\\site-packages (from swifter) (7.0.0)\n",
      "Requirement already satisfied: dask>=2.10.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2025.12.0)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from swifter) (4.67.1)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.12.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ee hann\\appdata\\roaming\\python\\python312\\site-packages (from click>=8.1->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.4.6)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (20.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ee hann\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.0.0->swifter) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.0->swifter) (2025.2)\n",
      "Requirement already satisfied: locket in c:\\users\\ee hann\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ee hann\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a78ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸš€ VOYAGE DELAY PREDICTION - DATA INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Configuration:\n",
      "   Training years: [2020, 2021, 2022, 2023]\n",
      "   Test years: [2024]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CORRECTED DATA INTEGRATION PIPELINE\n",
    "Training: 2020-2023 | Testing: 2024\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ VOYAGE DELAY PREDICTION - DATA INTEGRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TRAIN_YEARS = [2020, 2021, 2022, 2023]\n",
    "TEST_YEARS = [2024]\n",
    "\n",
    "WEATHER_BUFFER = 5\n",
    "NEWS_BUFFER = 10\n",
    "\n",
    "OUTPUT_TRAIN = 'train_2020_2023.csv'\n",
    "OUTPUT_TEST = 'test_2024.csv'\n",
    "PREPROCESSING_PARAMS = 'preprocessing_params.pkl'\n",
    "\n",
    "print(f\"\\nğŸ“ Configuration:\")\n",
    "print(f\"   Training years: {TRAIN_YEARS}\")\n",
    "print(f\"   Test years: {TEST_YEARS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1e7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: LOADING TRAINING DATA (2020-2023)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”µ TRAINING DATA:\n",
      "ğŸ“‚ Loading ais_data_2020_cleaned.csv...\n",
      "   âœ… 296,850 voyages\n",
      "ğŸ“‚ Loading ais_data_2021_cleaned.csv...\n",
      "   âœ… 324,672 voyages\n",
      "ğŸ“‚ Loading ais_data_2022_cleaned.csv...\n",
      "   âœ… 334,365 voyages\n",
      "ğŸ“‚ Loading ais_data_2023_cleaned.csv...\n",
      "   âœ… 352,334 voyages\n",
      "ğŸ“‚ Loading era5_weather_2020_sampled.csv...\n",
      "   âœ… 1,354,200 records\n",
      "ğŸ“‚ Loading era5_weather_2021_sampled.csv...\n",
      "   âœ… 1,350,500 records\n",
      "ğŸ“‚ Loading era5_weather_2022_sampled.csv...\n",
      "   âœ… 1,350,500 records\n",
      "ğŸ“‚ Loading era5_weather_2023_sampled.csv...\n",
      "   âœ… 1,350,500 records\n",
      "ğŸ“‚ Loading gdelt_events_clean_2020.csv...\n",
      "   âœ… 442,397 events\n",
      "ğŸ“‚ Loading gdelt_events_clean_2021.csv...\n",
      "   âœ… 405,830 events\n",
      "ğŸ“‚ Loading gdelt_events_clean_2022.csv...\n",
      "   âœ… 323,349 events\n",
      "ğŸ“‚ Loading gdelt_events_clean_2023.csv...\n",
      "   âœ… 445,836 events\n",
      "\n",
      "âœ… Training data loaded:\n",
      "   AIS: 1,308,221 voyages\n",
      "   Weather: 5,405,700 records\n",
      "   GDELT: 1,617,412 events\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND COMBINE TRAINING DATA (2020-2023)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: LOADING TRAINING DATA (2020-2023)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def load_ais_data(years):\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        filename = f'ais_data_{year}_cleaned.csv'\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"ğŸ“‚ Loading {filename}...\")\n",
    "            df = pd.read_csv(filename, parse_dates=['StartTime', 'EndTime'],\n",
    "                           dtype={'MMSI': 'str', 'VesselName': 'str', 'IMO': 'str'})\n",
    "            dfs.append(df)\n",
    "            print(f\"   âœ… {len(df):,} voyages\")\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def load_weather_data(years):\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        filename = f'era5_weather_{year}_sampled.csv'\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"ğŸ“‚ Loading {filename}...\")\n",
    "            df = pd.read_csv(filename, parse_dates=['time'])\n",
    "            dfs.append(df)\n",
    "            print(f\"   âœ… {len(df):,} records\")\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def load_gdelt_data(years):\n",
    "    dfs = []\n",
    "    for year in years:\n",
    "        filename = f'gdelt_events_clean_{year}.csv'\n",
    "        if os.path.exists(filename):\n",
    "            print(f\"ğŸ“‚ Loading {filename}...\")\n",
    "            df = pd.read_csv(filename, parse_dates=['Date'])\n",
    "            dfs.append(df)\n",
    "            print(f\"   âœ… {len(df):,} events\")\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "print(\"\\nğŸ”µ TRAINING DATA:\")\n",
    "train_ais = load_ais_data(TRAIN_YEARS)\n",
    "train_weather = load_weather_data(TRAIN_YEARS)\n",
    "train_gdelt = load_gdelt_data(TRAIN_YEARS)\n",
    "\n",
    "print(f\"\\nâœ… Training data loaded:\")\n",
    "print(f\"   AIS: {len(train_ais):,} voyages\")\n",
    "print(f\"   Weather: {len(train_weather):,} records\")\n",
    "print(f\"   GDELT: {len(train_gdelt):,} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a5b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: PREPROCESSING TRAINING DATA\n",
      "======================================================================\n",
      "\n",
      "ğŸŒ¦ï¸  Preprocessing weather data...\n",
      "   âœ… Weather preprocessing complete: 5,405,700 records\n",
      "\n",
      "ğŸ“° Preprocessing GDELT data...\n",
      "   âœ… GDELT preprocessing complete: 1,617,412 events\n",
      "\n",
      "ğŸ’¾ Saving preprocessing parameters to: preprocessing_params.pkl\n",
      "   âœ… Saved!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESS TRAINING DATA & SAVE PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: PREPROCESSING TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "preprocessing_params = {}\n",
    "\n",
    "# ========== WEATHER PREPROCESSING ==========\n",
    "print(\"\\nğŸŒ¦ï¸  Preprocessing weather data...\")\n",
    "\n",
    "# Remove invalids\n",
    "train_weather = train_weather[\n",
    "    train_weather['latitude'].between(-90, 90) &\n",
    "    train_weather['longitude'].between(-180, 180)\n",
    "]\n",
    "\n",
    "# Calculate statistics ONLY on training data\n",
    "weather_means = train_weather.select_dtypes(include=[np.number]).mean()\n",
    "preprocessing_params['weather_means'] = weather_means\n",
    "\n",
    "# Fill missing values\n",
    "train_weather = train_weather.fillna(weather_means)\n",
    "\n",
    "# Calculate outlier bounds ONLY on training data\n",
    "weather_cols = ['wind_speed', 't2m', 'sp', 'msl', 'tp']\n",
    "outlier_bounds = {}\n",
    "for col in weather_cols:\n",
    "    Q1 = train_weather[col].quantile(0.01)\n",
    "    Q3 = train_weather[col].quantile(0.99)\n",
    "    outlier_bounds[col] = (Q1, Q3)\n",
    "    train_weather[col] = train_weather[col].clip(Q1, Q3)\n",
    "preprocessing_params['weather_outlier_bounds'] = outlier_bounds\n",
    "\n",
    "# Feature engineering\n",
    "train_weather['temp_celsius'] = train_weather['t2m'] - 273.15\n",
    "train_weather['pressure_hpa'] = train_weather['sp'] / 100\n",
    "train_weather['hour'] = train_weather['time'].dt.hour\n",
    "train_weather['day_of_year'] = train_weather['time'].dt.dayofyear\n",
    "\n",
    "print(f\"   âœ… Weather preprocessing complete: {len(train_weather):,} records\")\n",
    "\n",
    "# ========== GDELT PREPROCESSING ==========\n",
    "print(\"\\nğŸ“° Preprocessing GDELT data...\")\n",
    "\n",
    "train_gdelt['Actor1CountryCode'] = train_gdelt['Actor1CountryCode'].fillna('UNKNOWN')\n",
    "train_gdelt = train_gdelt[\n",
    "    train_gdelt['GoldsteinScale'].between(-10, 10) &\n",
    "    train_gdelt['ActionGeo_Lat'].between(-90, 90) &\n",
    "    train_gdelt['ActionGeo_Long'].between(-180, 180)\n",
    "]\n",
    "\n",
    "def categorize_event(goldstein):\n",
    "    if goldstein < -5: return 'Very Negative'\n",
    "    elif goldstein < 0: return 'Negative'\n",
    "    elif goldstein == 0: return 'Neutral'\n",
    "    elif goldstein < 5: return 'Positive'\n",
    "    else: return 'Very Positive'\n",
    "\n",
    "train_gdelt['EventCategory'] = train_gdelt['GoldsteinScale'].apply(categorize_event)\n",
    "\n",
    "print(f\"   âœ… GDELT preprocessing complete: {len(train_gdelt):,} events\")\n",
    "\n",
    "# Save preprocessing parameters\n",
    "print(f\"\\nğŸ’¾ Saving preprocessing parameters to: {PREPROCESSING_PARAMS}\")\n",
    "with open(PREPROCESSING_PARAMS, 'wb') as f:\n",
    "    pickle.dump(preprocessing_params, f)\n",
    "print(\"   âœ… Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e76956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: FEATURE ENGINEERING - TRAINING DATA\n",
      "======================================================================\n",
      "\n",
      "ğŸ”§ Creating spatial-temporal indices for faster lookups...\n",
      "   âœ… Indices created\n",
      "\n",
      "ğŸ”„ Processing 1,308,221 training voyages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1308221/1308221 [16:22:24<00:00, 22.19it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Feature extraction complete!\n",
      "   Total features extracted: 18\n",
      "\n",
      "ğŸ§­ Creating route-based features and target variables...\n",
      "   âœ… Calculated baselines for 1494 unique routes\n",
      "   ğŸš€ Mapping route baselines...\n",
      "   ğŸ” Validation:\n",
      "      Matched routes: 1,308,221 (100.0%)\n",
      "      Using fallback: 0 (0.0%)\n",
      "      âœ… All expected durations valid\n",
      "      ğŸ“Š Duration range: 0.6h - 744.0h\n",
      "   âœ… Target variables created with route-based baselines\n",
      "   ğŸ“Š New class distribution:\n",
      "      Not Delayed: 975,604 (74.6%)\n",
      "      Delayed: 332,617 (25.4%)\n",
      "   âœ… All features created\n",
      "\n",
      "ğŸ’¾ Saving training data to: train_2020_2023.csv\n",
      "   âœ… Saved successfully!\n",
      "   ğŸ“Š Rows: 1,308,221\n",
      "   ğŸ“‹ Columns: 49\n",
      "   ğŸ“ File size: 441.3 MB\n",
      "   ğŸ¯ Delayed voyages: 332,617 (25.4%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING FOR TRAINING DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: FEATURE ENGINEERING - TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========== STEP 3.1: CREATE SPATIAL-TEMPORAL INDICES ==========\n",
    "print(\"\\nğŸ”§ Creating spatial-temporal indices for faster lookups...\")\n",
    "\n",
    "# Sort and create time bins for weather data\n",
    "train_weather = train_weather.sort_values('time').reset_index(drop=True)\n",
    "train_weather['year_month'] = train_weather['time'].dt.to_period('M')\n",
    "\n",
    "# Create spatial bins for weather (1-degree grid)\n",
    "train_weather['lat_bin'] = (train_weather['latitude'] // 1).astype(int)\n",
    "train_weather['lon_bin'] = (train_weather['longitude'] // 1).astype(int)\n",
    "\n",
    "# Sort and create date bins for GDELT\n",
    "train_gdelt = train_gdelt.sort_values('Date').reset_index(drop=True)\n",
    "train_gdelt['year_month'] = train_gdelt['Date'].dt.to_period('M')\n",
    "\n",
    "# Create spatial bins for GDELT (1-degree grid)\n",
    "train_gdelt['lat_bin'] = (train_gdelt['ActionGeo_Lat'] // 1).astype(int)\n",
    "train_gdelt['lon_bin'] = (train_gdelt['ActionGeo_Long'] // 1).astype(int)\n",
    "\n",
    "print(\"   âœ… Indices created\")\n",
    "\n",
    "# ========== STEP 3.2: FEATURE EXTRACTION FUNCTION ==========\n",
    "\n",
    "def extract_voyage_features_optimized(row):\n",
    "    \"\"\"Extract weather and news features along ACTUAL route (start â†’ end)\"\"\"\n",
    "\n",
    "    start = row['StartTime']\n",
    "    end = row['EndTime']\n",
    "    start_lat, start_lon = row['StartLatitude'], row['StartLongitude']\n",
    "    end_lat, end_lon = row['EndLatitude'], row['EndLongitude']\n",
    "\n",
    "    # Create bounding box along ACTUAL route\n",
    "    min_lat = min(start_lat, end_lat) - WEATHER_BUFFER\n",
    "    max_lat = max(start_lat, end_lat) + WEATHER_BUFFER\n",
    "    min_lon = min(start_lon, end_lon) - WEATHER_BUFFER\n",
    "    max_lon = max(start_lon, end_lon) + WEATHER_BUFFER\n",
    "\n",
    "    # ========== WEATHER FEATURES ==========\n",
    "    # Step 1: Filter by time period (coarse filter)\n",
    "    voyage_period = start.to_period('M')\n",
    "    period_range = [voyage_period]\n",
    "    if start.to_period('M') != end.to_period('M'):\n",
    "        period_range.append(end.to_period('M'))\n",
    "    \n",
    "    weather_time_filtered = train_weather[\n",
    "        train_weather['year_month'].isin(period_range)\n",
    "    ]\n",
    "    \n",
    "    # Step 2: Filter by spatial bins (coarse filter)\n",
    "    lat_bins = range(int(min_lat) - 1, int(max_lat) + 2)\n",
    "    lon_bins = range(int(min_lon) - 1, int(max_lon) + 2)\n",
    "    \n",
    "    if len(weather_time_filtered) > 0:\n",
    "        weather_spatial_filtered = weather_time_filtered[\n",
    "            weather_time_filtered['lat_bin'].isin(lat_bins) &\n",
    "            weather_time_filtered['lon_bin'].isin(lon_bins)\n",
    "        ]\n",
    "        \n",
    "        # Step 3: Fine-grained filter (only on subset)\n",
    "        if len(weather_spatial_filtered) > 0:\n",
    "            weather_mask = (\n",
    "                (weather_spatial_filtered['time'] >= start) & \n",
    "                (weather_spatial_filtered['time'] <= end) &\n",
    "                (weather_spatial_filtered['latitude'].between(min_lat, max_lat)) &\n",
    "                (weather_spatial_filtered['longitude'].between(min_lon, max_lon))\n",
    "            )\n",
    "            voyage_weather = weather_spatial_filtered[weather_mask]\n",
    "        else:\n",
    "            voyage_weather = pd.DataFrame()\n",
    "    else:\n",
    "        voyage_weather = pd.DataFrame()\n",
    "    \n",
    "    # Extract weather features\n",
    "    if len(voyage_weather) > 0:\n",
    "        weather_features = {\n",
    "            'avg_wind_speed': voyage_weather['wind_speed'].mean(),\n",
    "            'max_wind_speed': voyage_weather['wind_speed'].max(),\n",
    "            'min_wind_speed': voyage_weather['wind_speed'].min(),\n",
    "            'avg_temp_celsius': voyage_weather['temp_celsius'].mean(),\n",
    "            'max_temp_celsius': voyage_weather['temp_celsius'].max(),\n",
    "            'min_temp_celsius': voyage_weather['temp_celsius'].min(),\n",
    "            'avg_pressure_hpa': voyage_weather['pressure_hpa'].mean(),\n",
    "            'total_precipitation': voyage_weather['tp'].sum(),\n",
    "            'weather_records': len(voyage_weather)\n",
    "        }\n",
    "    else:\n",
    "        weather_features = {\n",
    "            'avg_wind_speed': np.nan, 'max_wind_speed': np.nan, 'min_wind_speed': np.nan,\n",
    "            'avg_temp_celsius': np.nan, 'max_temp_celsius': np.nan, 'min_temp_celsius': np.nan,\n",
    "            'avg_pressure_hpa': np.nan, 'total_precipitation': np.nan, 'weather_records': 0\n",
    "        }\n",
    "    \n",
    "    # ========== NEWS FEATURES ==========\n",
    "    min_lat_news = min(start_lat, end_lat) - NEWS_BUFFER\n",
    "    max_lat_news = max(start_lat, end_lat) + NEWS_BUFFER\n",
    "    min_lon_news = min(start_lon, end_lon) - NEWS_BUFFER\n",
    "    max_lon_news = max(start_lon, end_lon) + NEWS_BUFFER\n",
    "    \n",
    "    # Step 1: Filter by time period\n",
    "    gdelt_time_filtered = train_gdelt[\n",
    "        train_gdelt['year_month'].isin(period_range)\n",
    "    ]\n",
    "    \n",
    "    # Step 2: Filter by spatial bins\n",
    "    lat_bins_news = range(int(min_lat_news) - 1, int(max_lat_news) + 2)\n",
    "    lon_bins_news = range(int(min_lon_news) - 1, int(max_lon_news) + 2)\n",
    "    \n",
    "    if len(gdelt_time_filtered) > 0:\n",
    "        gdelt_spatial_filtered = gdelt_time_filtered[\n",
    "            gdelt_time_filtered['lat_bin'].isin(lat_bins_news) &\n",
    "            gdelt_time_filtered['lon_bin'].isin(lon_bins_news)\n",
    "        ]\n",
    "        \n",
    "        # Step 3: Fine-grained filter\n",
    "        if len(gdelt_spatial_filtered) > 0:\n",
    "            news_mask = (\n",
    "                (gdelt_spatial_filtered['Date'] >= start) & \n",
    "                (gdelt_spatial_filtered['Date'] <= end) &\n",
    "                (gdelt_spatial_filtered['ActionGeo_Lat'].between(min_lat_news, max_lat_news)) &\n",
    "                (gdelt_spatial_filtered['ActionGeo_Long'].between(min_lon_news, max_lon_news))\n",
    "            )\n",
    "            voyage_news = gdelt_spatial_filtered[news_mask]\n",
    "        else:\n",
    "            voyage_news = pd.DataFrame()\n",
    "    else:\n",
    "        voyage_news = pd.DataFrame()\n",
    "    \n",
    "    # Extract news features\n",
    "    if len(voyage_news) > 0:\n",
    "        news_features = {\n",
    "            'num_events': len(voyage_news),\n",
    "            'avg_goldstein': voyage_news['GoldsteinScale'].mean(),\n",
    "            'min_goldstein': voyage_news['GoldsteinScale'].min(),\n",
    "            'max_goldstein': voyage_news['GoldsteinScale'].max(),\n",
    "            'avg_tone': voyage_news['AvgTone'].mean(),\n",
    "            'total_mentions': voyage_news['NumMentions'].sum(),\n",
    "            'total_sources': voyage_news['NumSources'].sum(),\n",
    "            'negative_events': (voyage_news['GoldsteinScale'] < 0).sum(),\n",
    "            'positive_events': (voyage_news['GoldsteinScale'] > 0).sum(),\n",
    "        }\n",
    "    else:\n",
    "        news_features = {\n",
    "            'num_events': 0, 'avg_goldstein': 0, 'min_goldstein': 0, 'max_goldstein': 0,\n",
    "            'avg_tone': 0, 'total_mentions': 0, 'total_sources': 0,\n",
    "            'negative_events': 0, 'positive_events': 0\n",
    "        }\n",
    "    \n",
    "    return {**weather_features, **news_features}\n",
    "\n",
    "# ========== STEP 3.3: FUNCTION PROCESSING USING SWIFTER ==========\n",
    "\n",
    "import swifter\n",
    "\n",
    "# ============= TEST SAMPLE RUN =============\n",
    "'''\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "print(f\"\\nğŸ§ª TESTING MODE: Processing {SAMPLE_SIZE:,} sample voyages...\")\n",
    "\n",
    "train_ais_sample = train_ais.head(SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"   Original dataset: {len(train_ais):,} voyages\")\n",
    "print(f\"   Sample size: {len(train_ais_sample):,} voyages ({len(train_ais_sample)/len(train_ais)*100:.1f}%)\")\n",
    "\n",
    "# Process sample\n",
    "train_features = train_ais_sample.swifter.apply(\n",
    "    lambda row: extract_voyage_features_optimized(row),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Combine with original AIS data\n",
    "train_df = pd.concat([train_ais_sample.reset_index(drop=True), train_features.reset_index(drop=True)], axis=1)\n",
    "'''\n",
    "# ============= PRODUCTION RUN =============\n",
    "\n",
    "print(f\"\\nğŸ”„ Processing {len(train_ais):,} training voyages...\")\n",
    "\n",
    "# Use swifter\n",
    "train_features = train_ais.swifter.apply(\n",
    "    lambda row: extract_voyage_features_optimized(row),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Combine with original AIS data\n",
    "train_df = pd.concat([train_ais.reset_index(drop=True), train_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(f\"\\nâœ… Feature extraction complete!\")\n",
    "print(f\"   Total features extracted: {train_features.shape[1]}\")\n",
    "\n",
    "# ========== STEP 3.4: CREATE TARGET VARIABLES AND ADDITIONAL FEATURES ==========\n",
    "\n",
    "# Route-based delay definition\n",
    "print(\"\\nğŸ§­ Creating route-based features and target variables...\")\n",
    "\n",
    "# Create start and end region bins (10-degree grid for broader regions)\n",
    "train_df['start_lat_region'] = (train_df['StartLatitude'] // 10).astype(int)\n",
    "train_df['start_lon_region'] = (train_df['StartLongitude'] // 10).astype(int)\n",
    "train_df['end_lat_region'] = (train_df['EndLatitude'] // 10).astype(int)\n",
    "train_df['end_lon_region'] = (train_df['EndLongitude'] // 10).astype(int)\n",
    "\n",
    "# Calculate route-specific baseline durations (median per route)\n",
    "route_baseline_durations = train_df.groupby([\n",
    "    'start_lat_region', 'start_lon_region',\n",
    "    'end_lat_region', 'end_lon_region'\n",
    "])['DurationHours'].quantile(0.5).to_dict()  # 50th percentile\n",
    "\n",
    "# Save to preprocessing params for test set\n",
    "preprocessing_params['route_baseline_durations'] = route_baseline_durations\n",
    "print(f\"   âœ… Calculated baselines for {len(route_baseline_durations)} unique routes\")\n",
    "\n",
    "# Calculate expected duration for each voyage based on its route\n",
    "print(\"   ğŸš€ Mapping route baselines...\")\n",
    "\n",
    "# Convert dictionary to DataFrame for fast merging\n",
    "route_baseline_df = pd.DataFrame([\n",
    "    {\n",
    "        'start_lat_region': k[0],\n",
    "        'start_lon_region': k[1], \n",
    "        'end_lat_region': k[2],\n",
    "        'end_lon_region': k[3],\n",
    "        'route_baseline': v\n",
    "    }\n",
    "    for k, v in route_baseline_durations.items()\n",
    "])\n",
    "\n",
    "# Vectorized merge to assign route baselines\n",
    "train_df = train_df.merge(\n",
    "    route_baseline_df,\n",
    "    on=['start_lat_region', 'start_lon_region', 'end_lat_region', 'end_lon_region'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# If route not seen before, use overall median duration\n",
    "global_median = train_df['DurationHours'].median()\n",
    "train_df['ExpectedDuration'] = train_df['route_baseline'].fillna(global_median)\n",
    "train_df = train_df.drop('route_baseline', axis=1)\n",
    "\n",
    "# Validation checks\n",
    "print(\"   ğŸ” Validation:\")\n",
    "missing_routes = train_df['ExpectedDuration'].isna().sum()\n",
    "print(f\"      Matched routes: {len(train_df) - missing_routes:,} ({(1-missing_routes/len(train_df))*100:.1f}%)\")\n",
    "print(f\"      Using fallback: {missing_routes:,} ({missing_routes/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Sanity check: Expected duration should be reasonable\n",
    "invalid_durations = (train_df['ExpectedDuration'] < 0) | (train_df['ExpectedDuration'] > 10000)\n",
    "if invalid_durations.any():\n",
    "    print(f\"      âš ï¸  WARNING: {invalid_durations.sum()} invalid expected durations!\")\n",
    "else:\n",
    "    print(f\"      âœ… All expected durations valid\")\n",
    "    print(f\"      ğŸ“Š Duration range: {train_df['ExpectedDuration'].min():.1f}h - {train_df['ExpectedDuration'].max():.1f}h\")\n",
    "\n",
    "# Target variables\n",
    "train_df['DurationDifference'] = train_df['DurationHours'] - train_df['ExpectedDuration']\n",
    "train_df['DelayPercentage'] = (train_df['DurationDifference'] / train_df['ExpectedDuration']) * 100\n",
    "train_df['IsDelayed'] = (train_df['DelayPercentage'] > 30).astype(int)  # 30% threshold\n",
    "train_df['DelayHours'] = train_df['DurationDifference']\n",
    "\n",
    "print(f\"   âœ… Target variables created with route-based baselines\")\n",
    "print(f\"   ğŸ“Š New class distribution:\")\n",
    "print(f\"      Not Delayed: {(train_df['IsDelayed']==0).sum():,} ({(train_df['IsDelayed']==0).mean()*100:.1f}%)\")\n",
    "print(f\"      Delayed: {(train_df['IsDelayed']==1).sum():,} ({(train_df['IsDelayed']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Temporal features\n",
    "train_df['StartMonth'] = train_df['StartTime'].dt.month\n",
    "train_df['StartDayOfWeek'] = train_df['StartTime'].dt.dayofweek\n",
    "train_df['StartHour'] = train_df['StartTime'].dt.hour\n",
    "train_df['StartQuarter'] = train_df['StartTime'].dt.quarter\n",
    "train_df['StartYear'] = train_df['StartTime'].dt.year\n",
    "\n",
    "# Weather severity flags\n",
    "train_df['HighWindFlag'] = (train_df['max_wind_speed'] > 15).astype(int)\n",
    "train_df['HeavyPrecipitationFlag'] = (train_df['total_precipitation'] > 0.01).astype(int)\n",
    "\n",
    "# News sentiment indicators\n",
    "train_df['HasNegativeNews'] = (train_df['negative_events'] > 0).astype(int)\n",
    "train_df['NegativeNewsRatio'] = train_df['negative_events'] / (train_df['num_events'] + 1)\n",
    "\n",
    "# Fill missing values\n",
    "weather_news_cols = [col for col in train_df.columns if col.startswith(('avg_', 'max_', 'min_', 'total_'))]\n",
    "train_df[weather_news_cols] = train_df[weather_news_cols].fillna(0)\n",
    "\n",
    "print(\"   âœ… All features created\")\n",
    "\n",
    "# ========== STEP 3.5: SAVE TRAINING DATA ==========\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saving training data to: {OUTPUT_TRAIN}\")\n",
    "train_df.to_csv(OUTPUT_TRAIN, index=False)\n",
    "\n",
    "file_size = os.path.getsize(OUTPUT_TRAIN) / (1024**2)\n",
    "print(f\"   âœ… Saved successfully!\")\n",
    "print(f\"   ğŸ“Š Rows: {len(train_df):,}\")\n",
    "print(f\"   ğŸ“‹ Columns: {train_df.shape[1]}\")\n",
    "print(f\"   ğŸ“ File size: {file_size:.1f} MB\")\n",
    "print(f\"   ğŸ¯ Delayed voyages: {train_df['IsDelayed'].sum():,} ({train_df['IsDelayed'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91aef02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: PROCESSING TEST DATA (2024)\n",
      "======================================================================\n",
      "\n",
      "ğŸ”´ TEST DATA:\n",
      "ğŸ“‚ Loading ais_data_2024_cleaned.csv...\n",
      "   âœ… 367,077 voyages\n",
      "ğŸ“‚ Loading era5_weather_2024_sampled.csv...\n",
      "   âœ… 1,354,200 records\n",
      "ğŸ“‚ Loading gdelt_events_clean_2024.csv...\n",
      "   âœ… 466,495 events\n",
      "\n",
      "âœ… Test data loaded:\n",
      "   AIS: 367,077 voyages\n",
      "   Weather: 1,354,200 records\n",
      "   GDELT: 466,495 events\n",
      "\n",
      "ğŸ“‹ Loading preprocessing parameters...\n",
      "\n",
      "ğŸŒ¦ï¸  Applying weather preprocessing...\n",
      "   âœ… Weather preprocessing complete: 1,354,200 records\n",
      "\n",
      "ğŸ“° Applying GDELT preprocessing...\n",
      "   âœ… GDELT preprocessing complete: 466,495 events\n",
      "\n",
      "ğŸ”§ Creating spatial-temporal indices for test data...\n",
      "   âœ… Test indices created\n",
      "\n",
      "ğŸ”„ Processing 367,077 test voyages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 367077/367077 [2:18:39<00:00, 44.12it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§­ Creating route-based features for test set...\n",
      "   ğŸš€ Mapping route baselines for test set...\n",
      "   âš ï¸  Found 52 voyages with unseen routes\n",
      "   ğŸ” Validation:\n",
      "      Matched training routes: 367,025 (100.0%)\n",
      "      Unseen routes (fallback): 52 (0.0%)\n",
      "      âœ… All expected durations valid\n",
      "      ğŸ“Š Duration range: 0.6h - 743.9h\n",
      "   âœ… Test set targets created\n",
      "   ğŸ“Š Class distribution:\n",
      "      Not Delayed: 275,022 (74.9%)\n",
      "      Delayed: 92,055 (25.1%)\n",
      "\n",
      "ğŸ’¾ Saving test data to: test_2024.csv\n",
      "   âœ… Saved 367,077 rows, 49 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND PREPROCESS TEST DATA (2024)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: PROCESSING TEST DATA (2024)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ”´ TEST DATA:\")\n",
    "test_ais = load_ais_data(TEST_YEARS)\n",
    "test_weather = load_weather_data(TEST_YEARS)\n",
    "test_gdelt = load_gdelt_data(TEST_YEARS)\n",
    "\n",
    "print(f\"\\nâœ… Test data loaded:\")\n",
    "print(f\"   AIS: {len(test_ais):,} voyages\")\n",
    "print(f\"   Weather: {len(test_weather):,} records\")\n",
    "print(f\"   GDELT: {len(test_gdelt):,} events\")\n",
    "\n",
    "# ========== APPLY TRAINING PREPROCESSING TO TEST DATA ==========\n",
    "print(\"\\nğŸ“‹ Loading preprocessing parameters...\")\n",
    "with open(PREPROCESSING_PARAMS, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "print(\"\\nğŸŒ¦ï¸  Applying weather preprocessing...\")\n",
    "test_weather = test_weather[\n",
    "    test_weather['latitude'].between(-90, 90) &\n",
    "    test_weather['longitude'].between(-180, 180)\n",
    "]\n",
    "test_weather = test_weather.fillna(params['weather_means'])\n",
    "\n",
    "for col, (Q1, Q3) in params['weather_outlier_bounds'].items():\n",
    "    test_weather[col] = test_weather[col].clip(Q1, Q3)\n",
    "\n",
    "test_weather['temp_celsius'] = test_weather['t2m'] - 273.15\n",
    "test_weather['pressure_hpa'] = test_weather['sp'] / 100\n",
    "test_weather['hour'] = test_weather['time'].dt.hour\n",
    "test_weather['day_of_year'] = test_weather['time'].dt.dayofyear\n",
    "\n",
    "print(f\"   âœ… Weather preprocessing complete: {len(test_weather):,} records\")\n",
    "\n",
    "print(\"\\nğŸ“° Applying GDELT preprocessing...\")\n",
    "test_gdelt['Actor1CountryCode'] = test_gdelt['Actor1CountryCode'].fillna('UNKNOWN')\n",
    "test_gdelt = test_gdelt[\n",
    "    test_gdelt['GoldsteinScale'].between(-10, 10) &\n",
    "    test_gdelt['ActionGeo_Lat'].between(-90, 90) &\n",
    "    test_gdelt['ActionGeo_Long'].between(-180, 180)\n",
    "]\n",
    "test_gdelt['EventCategory'] = test_gdelt['GoldsteinScale'].apply(categorize_event)\n",
    "print(f\"   âœ… GDELT preprocessing complete: {len(test_gdelt):,} events\")\n",
    "\n",
    "# ========== CREATE INDICES FOR TEST DATA ==========\n",
    "print(\"\\nğŸ”§ Creating spatial-temporal indices for test data...\")\n",
    "\n",
    "test_weather = test_weather.sort_values('time').reset_index(drop=True)\n",
    "test_weather['year_month'] = test_weather['time'].dt.to_period('M')\n",
    "test_weather['lat_bin'] = (test_weather['latitude'] // 1).astype(int)\n",
    "test_weather['lon_bin'] = (test_weather['longitude'] // 1).astype(int)\n",
    "\n",
    "test_gdelt = test_gdelt.sort_values('Date').reset_index(drop=True)\n",
    "test_gdelt['year_month'] = test_gdelt['Date'].dt.to_period('M')\n",
    "test_gdelt['lat_bin'] = (test_gdelt['ActionGeo_Lat'] // 1).astype(int)\n",
    "test_gdelt['lon_bin'] = (test_gdelt['ActionGeo_Long'] // 1).astype(int)\n",
    "\n",
    "print(\"   âœ… Test indices created\")\n",
    "\n",
    "# ========== FEATURE EXTRACTION FUNCTION FOR TEST DATA ==========\n",
    "\n",
    "def extract_test_features_optimized(row):\n",
    "    \"\"\"Extract weather and news features along ACTUAL route for test data\"\"\"\n",
    "    \n",
    "    start = row['StartTime']\n",
    "    end = row['EndTime']\n",
    "    start_lat, start_lon = row['StartLatitude'], row['StartLongitude']\n",
    "    end_lat, end_lon = row['EndLatitude'], row['EndLongitude']\n",
    "    \n",
    "    # Create bounding box along ACTUAL route\n",
    "    min_lat = min(start_lat, end_lat) - WEATHER_BUFFER\n",
    "    max_lat = max(start_lat, end_lat) + WEATHER_BUFFER\n",
    "    min_lon = min(start_lon, end_lon) - WEATHER_BUFFER\n",
    "    max_lon = max(start_lon, end_lon) + WEATHER_BUFFER\n",
    "    \n",
    "    # Weather features\n",
    "    voyage_period = start.to_period('M')\n",
    "    period_range = [voyage_period]\n",
    "    if start.to_period('M') != end.to_period('M'):\n",
    "        period_range.append(end.to_period('M'))\n",
    "    \n",
    "    weather_time_filtered = test_weather[test_weather['year_month'].isin(period_range)]\n",
    "    \n",
    "    lat_bins = range(int(min_lat) - 1, int(max_lat) + 2)\n",
    "    lon_bins = range(int(min_lon) - 1, int(max_lon) + 2)\n",
    "    \n",
    "    if len(weather_time_filtered) > 0:\n",
    "        weather_spatial_filtered = weather_time_filtered[\n",
    "            weather_time_filtered['lat_bin'].isin(lat_bins) &\n",
    "            weather_time_filtered['lon_bin'].isin(lon_bins)\n",
    "        ]\n",
    "        \n",
    "        if len(weather_spatial_filtered) > 0:\n",
    "            weather_mask = (\n",
    "                (weather_spatial_filtered['time'] >= start) & \n",
    "                (weather_spatial_filtered['time'] <= end) &\n",
    "                (weather_spatial_filtered['latitude'].between(min_lat, max_lat)) &\n",
    "                (weather_spatial_filtered['longitude'].between(min_lon, max_lon))\n",
    "            )\n",
    "            voyage_weather = weather_spatial_filtered[weather_mask]\n",
    "        else:\n",
    "            voyage_weather = pd.DataFrame()\n",
    "    else:\n",
    "        voyage_weather = pd.DataFrame()\n",
    "    \n",
    "    if len(voyage_weather) > 0:\n",
    "        weather_features = {\n",
    "            'avg_wind_speed': voyage_weather['wind_speed'].mean(),\n",
    "            'max_wind_speed': voyage_weather['wind_speed'].max(),\n",
    "            'min_wind_speed': voyage_weather['wind_speed'].min(),\n",
    "            'avg_temp_celsius': voyage_weather['temp_celsius'].mean(),\n",
    "            'max_temp_celsius': voyage_weather['temp_celsius'].max(),\n",
    "            'min_temp_celsius': voyage_weather['temp_celsius'].min(),\n",
    "            'avg_pressure_hpa': voyage_weather['pressure_hpa'].mean(),\n",
    "            'total_precipitation': voyage_weather['tp'].sum(),\n",
    "            'weather_records': len(voyage_weather)\n",
    "        }\n",
    "    else:\n",
    "        weather_features = {\n",
    "            'avg_wind_speed': np.nan, 'max_wind_speed': np.nan, 'min_wind_speed': np.nan,\n",
    "            'avg_temp_celsius': np.nan, 'max_temp_celsius': np.nan, 'min_temp_celsius': np.nan,\n",
    "            'avg_pressure_hpa': np.nan, 'total_precipitation': np.nan, 'weather_records': 0\n",
    "        }\n",
    "    \n",
    "    # News features\n",
    "    min_lat_news = min(start_lat, end_lat) - NEWS_BUFFER\n",
    "    max_lat_news = max(start_lat, end_lat) + NEWS_BUFFER\n",
    "    min_lon_news = min(start_lon, end_lon) - NEWS_BUFFER\n",
    "    max_lon_news = max(start_lon, end_lon) + NEWS_BUFFER\n",
    "    \n",
    "    gdelt_time_filtered = test_gdelt[test_gdelt['year_month'].isin(period_range)]\n",
    "    \n",
    "    lat_bins_news = range(int(min_lat_news) - 1, int(max_lat_news) + 2)\n",
    "    lon_bins_news = range(int(min_lon_news) - 1, int(max_lon_news) + 2)\n",
    "    \n",
    "    if len(gdelt_time_filtered) > 0:\n",
    "        gdelt_spatial_filtered = gdelt_time_filtered[\n",
    "            gdelt_time_filtered['lat_bin'].isin(lat_bins_news) &\n",
    "            gdelt_time_filtered['lon_bin'].isin(lon_bins_news)\n",
    "        ]\n",
    "        \n",
    "        if len(gdelt_spatial_filtered) > 0:\n",
    "            news_mask = (\n",
    "                (gdelt_spatial_filtered['Date'] >= start) & \n",
    "                (gdelt_spatial_filtered['Date'] <= end) &\n",
    "                (gdelt_spatial_filtered['ActionGeo_Lat'].between(min_lat_news, max_lat_news)) &\n",
    "                (gdelt_spatial_filtered['ActionGeo_Long'].between(min_lon_news, max_lon_news))\n",
    "            )\n",
    "            voyage_news = gdelt_spatial_filtered[news_mask]\n",
    "        else:\n",
    "            voyage_news = pd.DataFrame()\n",
    "    else:\n",
    "        voyage_news = pd.DataFrame()\n",
    "    \n",
    "    if len(voyage_news) > 0:\n",
    "        news_features = {\n",
    "            'num_events': len(voyage_news),\n",
    "            'avg_goldstein': voyage_news['GoldsteinScale'].mean(),\n",
    "            'min_goldstein': voyage_news['GoldsteinScale'].min(),\n",
    "            'max_goldstein': voyage_news['GoldsteinScale'].max(),\n",
    "            'avg_tone': voyage_news['AvgTone'].mean(),\n",
    "            'total_mentions': voyage_news['NumMentions'].sum(),\n",
    "            'total_sources': voyage_news['NumSources'].sum(),\n",
    "            'negative_events': (voyage_news['GoldsteinScale'] < 0).sum(),\n",
    "            'positive_events': (voyage_news['GoldsteinScale'] > 0).sum(),\n",
    "        }\n",
    "    else:\n",
    "        news_features = {\n",
    "            'num_events': 0, 'avg_goldstein': 0, 'min_goldstein': 0, 'max_goldstein': 0,\n",
    "            'avg_tone': 0, 'total_mentions': 0, 'total_sources': 0,\n",
    "            'negative_events': 0, 'positive_events': 0\n",
    "        }\n",
    "    \n",
    "    return {**weather_features, **news_features}\n",
    "\n",
    "# ========== FEATURE ENGINEERING FOR TEST DATA ==========\n",
    "\n",
    "import swifter\n",
    "\n",
    "# ============= TEST SAMPLE RUN =============\n",
    "'''\n",
    "TEST_SAMPLE_SIZE = 10000\n",
    "test_ais_sample = test_ais.head(TEST_SAMPLE_SIZE).copy()\n",
    "\n",
    "print(f\"\\nğŸ”„ Processing {len(test_ais_sample):,} test voyages (sample)...\")\n",
    "test_features = test_ais_sample.swifter.apply(\n",
    "    lambda row: extract_test_features_optimized(row),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "test_df = pd.concat([test_ais_sample.reset_index(drop=True), test_features.reset_index(drop=True)], axis=1)\n",
    "'''\n",
    "# ============ PRODUCTION RUN =============\n",
    "\n",
    "print(f\"\\nğŸ”„ Processing {len(test_ais):,} test voyages...\")\n",
    "test_features = test_ais.swifter.apply(\n",
    "    lambda row: extract_test_features_optimized(row),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "test_df = pd.concat([test_ais.reset_index(drop=True), test_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Apply route-based approach (same as training)\n",
    "print(\"ğŸ§­ Creating route-based features for test set...\")\n",
    "\n",
    "# Create region bins\n",
    "test_df['start_lat_region'] = (test_df['StartLatitude'] // 10).astype(int)\n",
    "test_df['start_lon_region'] = (test_df['StartLongitude'] // 10).astype(int)\n",
    "test_df['end_lat_region'] = (test_df['EndLatitude'] // 10).astype(int)\n",
    "test_df['end_lon_region'] = (test_df['EndLongitude'] // 10).astype(int)\n",
    "\n",
    "# Calculate expected duration based on route\n",
    "print(\"   ğŸš€ Mapping route baselines for test set...\")\n",
    "\n",
    "# Use the same route_baseline_df from training\n",
    "test_df = test_df.merge(\n",
    "    route_baseline_df,\n",
    "    on=['start_lat_region', 'start_lon_region', 'end_lat_region', 'end_lon_region'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# For unseen routes, use median of all test voyages with similar start region\n",
    "unseen_routes_mask = test_df['route_baseline'].isna()\n",
    "unseen_count = unseen_routes_mask.sum()\n",
    "\n",
    "if unseen_count > 0:\n",
    "    print(f\"   âš ï¸  Found {unseen_count:,} voyages with unseen routes\")\n",
    "    \n",
    "    # Calculate start region medians for fallback\n",
    "    start_region_medians = test_df.groupby(\n",
    "        ['start_lat_region', 'start_lon_region']\n",
    "    )['DurationHours'].transform('median')\n",
    "    \n",
    "    # Apply fallback: start region median, then global median\n",
    "    test_df.loc[unseen_routes_mask, 'route_baseline'] = start_region_medians[unseen_routes_mask]\n",
    "    \n",
    "    # For routes with no similar start region, use global median\n",
    "    still_missing = test_df['route_baseline'].isna()\n",
    "    if still_missing.any():\n",
    "        test_df.loc[still_missing, 'route_baseline'] = test_df['DurationHours'].median()\n",
    "\n",
    "# Rename column\n",
    "test_df['ExpectedDuration'] = test_df['route_baseline']\n",
    "test_df = test_df.drop('route_baseline', axis=1)\n",
    "\n",
    "# Validation checks\n",
    "print(\"   ğŸ” Validation:\")\n",
    "matched_routes = len(test_df) - unseen_count\n",
    "print(f\"      Matched training routes: {matched_routes:,} ({matched_routes/len(test_df)*100:.1f}%)\")\n",
    "print(f\"      Unseen routes (fallback): {unseen_count:,} ({unseen_count/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "# Sanity check\n",
    "invalid_durations = (test_df['ExpectedDuration'] < 0) | (test_df['ExpectedDuration'] > 10000)\n",
    "if invalid_durations.any():\n",
    "    print(f\"      âš ï¸  WARNING: {invalid_durations.sum()} invalid expected durations!\")\n",
    "else:\n",
    "    print(f\"      âœ… All expected durations valid\")\n",
    "    print(f\"      ğŸ“Š Duration range: {test_df['ExpectedDuration'].min():.1f}h - {test_df['ExpectedDuration'].max():.1f}h\")\n",
    "\n",
    "# Target variables\n",
    "test_df['DurationDifference'] = test_df['DurationHours'] - test_df['ExpectedDuration']\n",
    "test_df['DelayPercentage'] = (test_df['DurationDifference'] / test_df['ExpectedDuration']) * 100\n",
    "test_df['IsDelayed'] = (test_df['DelayPercentage'] > 30).astype(int)  # Same 30% threshold\n",
    "test_df['DelayHours'] = test_df['DurationDifference']\n",
    "\n",
    "print(f\"   âœ… Test set targets created\")\n",
    "print(f\"   ğŸ“Š Class distribution:\")\n",
    "print(f\"      Not Delayed: {(test_df['IsDelayed']==0).sum():,} ({(test_df['IsDelayed']==0).mean()*100:.1f}%)\")\n",
    "print(f\"      Delayed: {(test_df['IsDelayed']==1).sum():,} ({(test_df['IsDelayed']==1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Additional features (same as training)\n",
    "test_df['StartMonth'] = test_df['StartTime'].dt.month\n",
    "test_df['StartDayOfWeek'] = test_df['StartTime'].dt.dayofweek\n",
    "test_df['StartHour'] = test_df['StartTime'].dt.hour\n",
    "test_df['StartQuarter'] = test_df['StartTime'].dt.quarter\n",
    "test_df['StartYear'] = test_df['StartTime'].dt.year\n",
    "test_df['HighWindFlag'] = (test_df['max_wind_speed'] > 15).astype(int)\n",
    "test_df['HeavyPrecipitationFlag'] = (test_df['total_precipitation'] > 0.01).astype(int)\n",
    "test_df['HasNegativeNews'] = (test_df['negative_events'] > 0).astype(int)\n",
    "test_df['NegativeNewsRatio'] = test_df['negative_events'] / (test_df['num_events'] + 1)\n",
    "\n",
    "# Fill missing values\n",
    "test_df[weather_news_cols] = test_df[weather_news_cols].fillna(0)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saving test data to: {OUTPUT_TEST}\")\n",
    "test_df.to_csv(OUTPUT_TEST, index=False)\n",
    "print(f\"   âœ… Saved {len(test_df):,} rows, {test_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec9cd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ‰ PIPELINE COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š TRAINING SET (2020-2023):\n",
      "   Voyages: 1,308,221\n",
      "   Delayed: 332,617 (25.4%)\n",
      "\n",
      "ğŸ“Š TEST SET (2024):\n",
      "   Voyages: 367,077\n",
      "   Delayed: 92,055 (25.1%)\n",
      "\n",
      "âœ… Next steps:\n",
      "   1. Load: train = pd.read_csv('train_2020_2023.csv')\n",
      "   2. Load: test = pd.read_csv('test_2024.csv')\n",
      "   3. Train models on train set\n",
      "   4. Evaluate on test set (2024)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ PIPELINE COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š TRAINING SET (2020-2023):\")\n",
    "print(f\"   Voyages: {len(train_df):,}\")\n",
    "print(f\"   Delayed: {train_df['IsDelayed'].sum():,} ({train_df['IsDelayed'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š TEST SET (2024):\")\n",
    "print(f\"   Voyages: {len(test_df):,}\")\n",
    "print(f\"   Delayed: {test_df['IsDelayed'].sum():,} ({test_df['IsDelayed'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Next steps:\")\n",
    "print(f\"   1. Load: train = pd.read_csv('{OUTPUT_TRAIN}')\")\n",
    "print(f\"   2. Load: test = pd.read_csv('{OUTPUT_TEST}')\")\n",
    "print(f\"   3. Train models on train set\")\n",
    "print(f\"   4. Evaluate on test set (2024)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
